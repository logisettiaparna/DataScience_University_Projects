{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PySpark Tutorial\n",
    "Deciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like:\n",
    "\n",
    "1. Is my data too big to work with on a single machine?\n",
    "2. Can my calculations be easily parallelized?\n",
    "\n",
    "PySpark is a great language for performing exploratory data analysis at scale, building machine learning pipelines, and creating ETLs for a data platform. If you’re already familiar with Python and libraries such as Pandas, then PySpark is a great language to learn in order to create more scalable analyses and pipelines. The goal of this post is to show how to get up and running with PySpark and to perform common tasks.\n",
    "\n",
    "The first step in using Spark is connecting to a cluster.\n",
    "\n",
    "In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called slaves. The master sends the slaves data and calculations to run, and they send their results back to the master.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, HiveContext, SparkContext\n",
    "from pyspark.sql import SparkSession, Row, SQLContext\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set Spark Context\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.memory','10g')\\\n",
    "    .set('spark.yarn.queue', 'root.odfgsbx2_q1')\\\n",
    "    .set('spark.executor.cores','5')\\\n",
    "    .set('spark.executor.instances','3')\\\n",
    "    \n",
    "spark = SparkSession.builder.appName('DSU') \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# Default Spark context\n",
    "#sc, sqlContext = setupSparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the parameters for the spark context: \n",
    "\n",
    "spark.executor.memory: heap size or memory for each executor <br /> <br />\n",
    "spark.yarn.queue : sending job to specified YARN Queue<br /> <br />\n",
    "spark.executor.cores : specify number of cores (controls number of concurrent tasks an executor can run), for the context above each executor can run a maximum of 5 tasks at the same time <br /> <br />\n",
    "spark.executor.instances : number of executors requested <br />\n",
    "\n",
    "https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/ <br />\n",
    "https://spark.apache.org/docs/latest/cluster-overview.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Py)Spark's core data structure and DataFrames\n",
    "\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are extremely hard to work with directly and so we  will use DataFrames.\n",
    "\n",
    "We learned about Data Frames towards the start of this course. These Spark DFs are really similar to pandas DF, except that they are on steroids and a few syntax is different. Spark DFs are optimized to work with RDD, so internally all the transformations and manipulations occur in Spark RDD.\n",
    "\n",
    "Spark dataframe can be thought of as a table distributed across a cluster and has functionality that is similar to dataframes in R and Pandas. If you want to do distributed computation using PySpark, then you’ll need to perform operations on Spark dataframes, and not other python data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, you will be reading in an external file. There are 2 ways of reading in a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0           1    Male   19                  15                      39\n",
       "1           2    Male   21                  15                      81\n",
       "2           3  Female   20                  16                       6\n",
       "3           4  Female   23                  16                      77\n",
       "4           5  Female   31                  17                      40"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Mall_Customers.csv')\n",
    "type(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://s3.amazonaws.com/assets.datacamp.com/production/course_4452/datasets/spark_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.createDataFrame()` method takes a pandas DF and returns a Spark DF.\n",
    "The output of this method is stored locally, not in the SparkSession catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts (unless you upload it).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a method to store things in SparkSession catalog in temp space.\n",
    "You can do this using the `.createTempView()` Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame. (Similar to sql temp tables)\n",
    "\n",
    "There is also the method `.createOrReplaceTempView()`. This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You'll use this method to avoid running into problems with duplicate tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         3|Female| 20|                16|                     6|\n",
      "|         4|Female| 23|                16|                    77|\n",
      "|         5|Female| 31|                17|                    40|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## read it in as a pandas and convert it to a pyspark dataframe\n",
    "spk_df = spark.createDataFrame(df)\n",
    "spk_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easily convert Spark DFs to pandas\n",
    "####  A word of caution, pandas can handle 10 mil rows, but the constraint is RAM. We don't want you to be the person blamed to bring down the BDPaaS queue :)\n",
    "This function should generally be avoided except when working with small dataframes, because it pulls the entire object into memory on a single node.\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0           1    Male   19                  15                      39\n",
       "1           2    Male   21                  15                      81\n",
       "2           3  Female   20                  16                       6\n",
       "3           4  Female   23                  16                      77\n",
       "4           5  Female   31                  17                      40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df = spk_df.toPandas()\n",
    "pd_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One of the key differences between Pandas and Spark dataframes is eager versus lazy execution. In PySpark, operations are delayed until a result is actually needed in the pipeline.\n",
    "\n",
    "For example, you can specify operations for loading a data set from S3 and applying a number of transformations to the dataframe, but these operations won’t immediately be applied. Instead, a graph of transformations is recorded, and once the data is actually needed, for example when writing the results back to S3, then the transformations are applied as a single pipeline operation. This approach is used to avoid pulling the full data frame into memory and enables more effective processing across a cluster of machines. With Pandas dataframes, everything is pulled into memory, and every Pandas operation is immediately applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a file (CSV, Parquets etc) directly into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mylist = [\n",
    "  {\"name\":'Alice',\"age\":\"10\"},\n",
    "  {\"name\":'Owen',\"age\":\"11\"},\n",
    "  {\"name\":'Ryan',\"age\":\"12\"}\n",
    "]\n",
    "\n",
    "local_df = spark.createDataFrame(Row(**x) for x in mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 10|Alice|\n",
      "| 11| Owen|\n",
      "| 12| Ryan|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_df.show()\n",
    "local_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read it in as a csv directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mall_customers_df = spark.read.csv(\"maprfs:///datalake/optum/optuminsight/sandbox3/dsu/nilay_bhatt/Mall_Customers.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         3|Female| 20|                16|                     6|\n",
      "|         4|Female| 23|                16|                    77|\n",
      "|         5|Female| 31|                17|                    40|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Annual Income (k$): string (nullable = true)\n",
      " |-- Spending Score (1-100): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mall_customers_df.show(5)\n",
    "mall_customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation: let the fun began!!!! \n",
    "#### A lot of things are similar from pandas but some nuances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things such as .columns, .describe, .head, are similar. We now have withColumns, UDFs, SparkSQL that we can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomerID', 'Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mall_customers_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "#### You can select columns either way, but the first way is slightly more common\n",
    "A large part of working with DataFrames is the ability to quickly filter out data based on conditions. Spark DataFrames are built on top of the Spark SQL platform, which means that is you already know SQL, you can quickly and easily grab that data using SQL commands, or using the DataFrame methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|Gender|Age|\n",
      "+------+---+\n",
      "|  Male| 19|\n",
      "|  Male| 21|\n",
      "|Female| 20|\n",
      "|Female| 23|\n",
      "|Female| 31|\n",
      "|Female| 22|\n",
      "|Female| 35|\n",
      "|Female| 23|\n",
      "|  Male| 64|\n",
      "|Female| 30|\n",
      "|  Male| 67|\n",
      "|Female| 35|\n",
      "|Female| 58|\n",
      "|Female| 24|\n",
      "|  Male| 37|\n",
      "|  Male| 22|\n",
      "|Female| 35|\n",
      "|  Male| 20|\n",
      "|  Male| 52|\n",
      "|Female| 35|\n",
      "+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mall_customers_df.select(F.col('Gender'), F.col('Age')).show()\n",
    "#df_trim_1 = mall_customers_df.select(df['Gender'], df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         3|Female| 20|                16|                     6|\n",
      "|         4|Female| 23|                16|                    77|\n",
      "|         5|Female| 31|                17|                    40|\n",
      "|         6|Female| 22|                17|                    76|\n",
      "|         7|Female| 35|                18|                     6|\n",
      "|         8|Female| 23|                18|                    94|\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        10|Female| 30|                19|                    72|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        12|Female| 35|                19|                    99|\n",
      "|        13|Female| 58|                20|                    15|\n",
      "|        14|Female| 24|                20|                    77|\n",
      "|        15|  Male| 37|                20|                    13|\n",
      "|        16|  Male| 22|                20|                    79|\n",
      "|        17|Female| 35|                21|                    35|\n",
      "|        18|  Male| 20|                21|                    66|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        20|Female| 35|                23|                    98|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mall_customers_df.filter(F.col('Age')>18).show()\n",
    "#mall_customers_df.filter('Age>18').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also chain functions. This is actually recomended way of working. Although as we are starting out, we might want to right out seperate commands, Spark also optimizes our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](DAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Gender|Annual Income (k$)|\n",
      "+------+------------------+\n",
      "|  Male|                15|\n",
      "|  Male|                15|\n",
      "|Female|                16|\n",
      "|Female|                16|\n",
      "|Female|                17|\n",
      "|Female|                17|\n",
      "|Female|                18|\n",
      "|Female|                18|\n",
      "|  Male|                19|\n",
      "|Female|                19|\n",
      "|  Male|                19|\n",
      "|Female|                19|\n",
      "|Female|                20|\n",
      "|Female|                20|\n",
      "|  Male|                20|\n",
      "|  Male|                20|\n",
      "|Female|                21|\n",
      "|  Male|                21|\n",
      "|  Male|                23|\n",
      "|Female|                23|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mall_customers_df.filter(F.col('Age')>18).select(F.col('Gender'), F.col('Annual Income (k$)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Gender|Annual Income (k$)|\n",
      "+------+------------------+\n",
      "|  Male|                15|\n",
      "|  Male|                15|\n",
      "|Female|                16|\n",
      "|Female|                16|\n",
      "|Female|                17|\n",
      "|Female|                17|\n",
      "|Female|                18|\n",
      "|Female|                18|\n",
      "|  Male|                19|\n",
      "|Female|                19|\n",
      "|  Male|                19|\n",
      "|Female|                19|\n",
      "|Female|                20|\n",
      "|Female|                20|\n",
      "|  Male|                20|\n",
      "|  Male|                20|\n",
      "|Female|                21|\n",
      "|  Male|                21|\n",
      "|  Male|                23|\n",
      "|Female|                23|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mall_customers_df.filter(F.col('Age')>18).select(['Gender','Annual Income (k$)']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using normal python comparison operators is another way to do this, they will look very similar to SQL operators, except you need to make sure you are calling the entire column within the dataframe, using the format: df[\"column name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        15|  Male| 37|                20|                    13|\n",
      "|        16|  Male| 22|                20|                    79|\n",
      "|        18|  Male| 20|                21|                    66|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        21|  Male| 35|                24|                    35|\n",
      "|        22|  Male| 25|                24|                    73|\n",
      "|        24|  Male| 31|                25|                    73|\n",
      "|        26|  Male| 29|                28|                    82|\n",
      "|        28|  Male| 35|                28|                    61|\n",
      "|        31|  Male| 60|                30|                     4|\n",
      "|        33|  Male| 53|                33|                     4|\n",
      "|        34|  Male| 18|                33|                    92|\n",
      "|        42|  Male| 24|                38|                    92|\n",
      "|        43|  Male| 48|                39|                    36|\n",
      "|        52|  Male| 33|                42|                    60|\n",
      "|        54|  Male| 59|                43|                    60|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mall_customers_df.filter(mall_customers_df[\"Gender\"] == 'Male').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        15|  Male| 37|                20|                    13|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        21|  Male| 35|                24|                    35|\n",
      "|        31|  Male| 60|                30|                     4|\n",
      "|        33|  Male| 53|                33|                     4|\n",
      "|        43|  Male| 48|                39|                    36|\n",
      "|        56|  Male| 47|                43|                    41|\n",
      "|        58|  Male| 69|                44|                    46|\n",
      "|        60|  Male| 53|                46|                    46|\n",
      "|        75|  Male| 59|                54|                    47|\n",
      "|        78|  Male| 40|                54|                    48|\n",
      "|        83|  Male| 67|                54|                    41|\n",
      "|        86|  Male| 48|                54|                    46|\n",
      "|        92|  Male| 18|                59|                    41|\n",
      "|        93|  Male| 48|                60|                    49|\n",
      "|        99|  Male| 48|                61|                    42|\n",
      "|       100|  Male| 20|                61|                    49|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mall_customers_df.filter((mall_customers_df[\"Gender\"] == \"Male\") & (mall_customers_df[\"Spending Score (1-100)\"] < 50)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CustomerID='9', Gender='Male', Age='64', Annual Income (k$)='19', Spending Score (1-100)='3'),\n",
       " Row(CustomerID='31', Gender='Male', Age='60', Annual Income (k$)='30', Spending Score (1-100)='4'),\n",
       " Row(CustomerID='33', Gender='Male', Age='53', Annual Income (k$)='33', Spending Score (1-100)='4'),\n",
       " Row(CustomerID='157', Gender='Male', Age='37', Annual Income (k$)='78', Spending Score (1-100)='1'),\n",
       " Row(CustomerID='159', Gender='Male', Age='34', Annual Income (k$)='78', Spending Score (1-100)='1')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mall_customers_df.filter((mall_customers_df[\"Gender\"] == \"Male\") & (mall_customers_df[\"Spending Score (1-100)\"] < 5)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows can be called to turn into dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also grab data by column or as a single column DF using df['Age'] or df.select('Age'). The latter gives a DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function ``.withColumn`` will create a new column if the name of the new column is different from the original columns. If the name of the new column is the same to an original column name, then that column will be replaced with the new values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = mall_customers_df.withColumn('Annual Income (k$)', F.col('Annual Income (k$)') * 1000)\n",
    "df = df.withColumnRenamed('Annual Income (k$)', 'Annual Income') ## Rename Annual Income (k$) to Annual Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------------+----------------------+\n",
      "|CustomerID|Gender|Age|Annual Income|Spending Score (1-100)|\n",
      "+----------+------+---+-------------+----------------------+\n",
      "|         1|  Male| 19|      15000.0|                    39|\n",
      "|         2|  Male| 21|      15000.0|                    81|\n",
      "|         3|Female| 20|      16000.0|                     6|\n",
      "|         4|Female| 23|      16000.0|                    77|\n",
      "|         5|Female| 31|      17000.0|                    40|\n",
      "|         6|Female| 22|      17000.0|                    76|\n",
      "|         7|Female| 35|      18000.0|                     6|\n",
      "|         8|Female| 23|      18000.0|                    94|\n",
      "|         9|  Male| 64|      19000.0|                     3|\n",
      "|        10|Female| 30|      19000.0|                    72|\n",
      "|        11|  Male| 67|      19000.0|                    14|\n",
      "|        12|Female| 35|      19000.0|                    99|\n",
      "|        13|Female| 58|      20000.0|                    15|\n",
      "|        14|Female| 24|      20000.0|                    77|\n",
      "|        15|  Male| 37|      20000.0|                    13|\n",
      "|        16|  Male| 22|      20000.0|                    79|\n",
      "|        17|Female| 35|      21000.0|                    35|\n",
      "|        18|  Male| 20|      21000.0|                    66|\n",
      "|        19|  Male| 52|      23000.0|                    29|\n",
      "|        20|Female| 35|      23000.0|                    98|\n",
      "+----------+------+---+-------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us create a new feature: annual income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('Annual_Income/Spending_Score', F.col('Annual Income')/F.col('Spending Score (1-100)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------------+----------------------+----------------------------+\n",
      "|CustomerID|Gender|Age|Annual Income|Spending Score (1-100)|Annual_Income/Spending_Score|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+\n",
      "|         1|  Male| 19|      15000.0|                    39|          384.61538461538464|\n",
      "|         2|  Male| 21|      15000.0|                    81|           185.1851851851852|\n",
      "|         3|Female| 20|      16000.0|                     6|          2666.6666666666665|\n",
      "|         4|Female| 23|      16000.0|                    77|           207.7922077922078|\n",
      "|         5|Female| 31|      17000.0|                    40|                       425.0|\n",
      "|         6|Female| 22|      17000.0|                    76|          223.68421052631578|\n",
      "|         7|Female| 35|      18000.0|                     6|                      3000.0|\n",
      "|         8|Female| 23|      18000.0|                    94|          191.48936170212767|\n",
      "|         9|  Male| 64|      19000.0|                     3|           6333.333333333333|\n",
      "|        10|Female| 30|      19000.0|                    72|           263.8888888888889|\n",
      "|        11|  Male| 67|      19000.0|                    14|           1357.142857142857|\n",
      "|        12|Female| 35|      19000.0|                    99|          191.91919191919192|\n",
      "|        13|Female| 58|      20000.0|                    15|          1333.3333333333333|\n",
      "|        14|Female| 24|      20000.0|                    77|           259.7402597402597|\n",
      "|        15|  Male| 37|      20000.0|                    13|          1538.4615384615386|\n",
      "|        16|  Male| 22|      20000.0|                    79|          253.16455696202533|\n",
      "|        17|Female| 35|      21000.0|                    35|                       600.0|\n",
      "|        18|  Male| 20|      21000.0|                    66|           318.1818181818182|\n",
      "|        19|  Male| 52|      23000.0|                    29|           793.1034482758621|\n",
      "|        20|Female| 35|      23000.0|                    98|          234.69387755102042|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us convert gender to binary / Normalizing the data\n",
    "\n",
    "### F.when() functions like an if/else statement. For example, if the gender is male then put the value equal to 0, else put the value to 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('Gender_Binary', F.when(F.col('Gender') == 'Male', 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+\n",
      "|CustomerID|Gender|Age|Annual Income|Spending Score (1-100)|Annual_Income/Spending_Score|Gender_Binary|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+\n",
      "|         1|  Male| 19|      15000.0|                    39|          384.61538461538464|            0|\n",
      "|         2|  Male| 21|      15000.0|                    81|           185.1851851851852|            0|\n",
      "|         3|Female| 20|      16000.0|                     6|          2666.6666666666665|            1|\n",
      "|         4|Female| 23|      16000.0|                    77|           207.7922077922078|            1|\n",
      "|         5|Female| 31|      17000.0|                    40|                       425.0|            1|\n",
      "|         6|Female| 22|      17000.0|                    76|          223.68421052631578|            1|\n",
      "|         7|Female| 35|      18000.0|                     6|                      3000.0|            1|\n",
      "|         8|Female| 23|      18000.0|                    94|          191.48936170212767|            1|\n",
      "|         9|  Male| 64|      19000.0|                     3|           6333.333333333333|            0|\n",
      "|        10|Female| 30|      19000.0|                    72|           263.8888888888889|            1|\n",
      "|        11|  Male| 67|      19000.0|                    14|           1357.142857142857|            0|\n",
      "|        12|Female| 35|      19000.0|                    99|          191.91919191919192|            1|\n",
      "|        13|Female| 58|      20000.0|                    15|          1333.3333333333333|            1|\n",
      "|        14|Female| 24|      20000.0|                    77|           259.7402597402597|            1|\n",
      "|        15|  Male| 37|      20000.0|                    13|          1538.4615384615386|            0|\n",
      "|        16|  Male| 22|      20000.0|                    79|          253.16455696202533|            0|\n",
      "|        17|Female| 35|      21000.0|                    35|                       600.0|            1|\n",
      "|        18|  Male| 20|      21000.0|                    66|           318.1818181818182|            0|\n",
      "|        19|  Male| 52|      23000.0|                    29|           793.1034482758621|            0|\n",
      "|        20|Female| 35|      23000.0|                    98|          234.69387755102042|            1|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can describe a dataframe similar to R to extract key statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+----------------------+------------------+\n",
      "|summary|        CustomerID|              Age|     Annual Income|Spending Score (1-100)|     Gender_Binary|\n",
      "+-------+------------------+-----------------+------------------+----------------------+------------------+\n",
      "|  count|               200|              200|               200|                   200|               200|\n",
      "|   mean|             100.5|            38.85|           60560.0|                  50.2|              0.56|\n",
      "| stddev|57.879184513951124|13.96900733155888|26264.721165271247|    25.823521668370173|0.4976325863071808|\n",
      "|    min|                 1|               18|           15000.0|                     1|                 0|\n",
      "|    max|                99|               70|          137000.0|                    99|                 1|\n",
      "+-------+------------------+-----------------+------------------+----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('Gender').drop('Annual_Income/Spending_Score').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs\n",
    "The following example is a UDF (user defined function). UDFs are means to apply Python functions to PySpark dataframes \n",
    "row by row. Before applying an UDF, make sure there are no pre-defined functions in PySpark already. UDFs are\n",
    "computationally expensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x): \n",
    "    return int(x)**2\n",
    "\n",
    "square_udf = F.udf(lambda z: square(z))\n",
    "\n",
    "df = df.withColumn('udf_test', square_udf(F.col('Spending Score (1-100)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+--------+\n",
      "|CustomerID|Gender|Age|Annual Income|Spending Score (1-100)|Annual_Income/Spending_Score|Gender_Binary|udf_test|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+--------+\n",
      "|         1|  Male| 19|      15000.0|                    39|          384.61538461538464|            0|    1521|\n",
      "|         2|  Male| 21|      15000.0|                    81|           185.1851851851852|            0|    6561|\n",
      "|         3|Female| 20|      16000.0|                     6|          2666.6666666666665|            1|      36|\n",
      "|         4|Female| 23|      16000.0|                    77|           207.7922077922078|            1|    5929|\n",
      "|         5|Female| 31|      17000.0|                    40|                       425.0|            1|    1600|\n",
      "|         6|Female| 22|      17000.0|                    76|          223.68421052631578|            1|    5776|\n",
      "|         7|Female| 35|      18000.0|                     6|                      3000.0|            1|      36|\n",
      "|         8|Female| 23|      18000.0|                    94|          191.48936170212767|            1|    8836|\n",
      "|         9|  Male| 64|      19000.0|                     3|           6333.333333333333|            0|       9|\n",
      "|        10|Female| 30|      19000.0|                    72|           263.8888888888889|            1|    5184|\n",
      "|        11|  Male| 67|      19000.0|                    14|           1357.142857142857|            0|     196|\n",
      "|        12|Female| 35|      19000.0|                    99|          191.91919191919192|            1|    9801|\n",
      "|        13|Female| 58|      20000.0|                    15|          1333.3333333333333|            1|     225|\n",
      "|        14|Female| 24|      20000.0|                    77|           259.7402597402597|            1|    5929|\n",
      "|        15|  Male| 37|      20000.0|                    13|          1538.4615384615386|            0|     169|\n",
      "|        16|  Male| 22|      20000.0|                    79|          253.16455696202533|            0|    6241|\n",
      "|        17|Female| 35|      21000.0|                    35|                       600.0|            1|    1225|\n",
      "|        18|  Male| 20|      21000.0|                    66|           318.1818181818182|            0|    4356|\n",
      "|        19|  Male| 52|      23000.0|                    29|           793.1034482758621|            0|     841|\n",
      "|        20|Female| 35|      23000.0|                    98|          234.69387755102042|            1|    9604|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "|CustomerID|Gender|Age|Annual_Income|Spending Score (1-100)|Annual_Income/Spending_Score|Gender_Binary|spend_square|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "|         1|  Male| 19|      15000.0|                    39|          384.61538461538464|            0|        1521|\n",
      "|         2|  Male| 21|      15000.0|                    81|           185.1851851851852|            0|        6561|\n",
      "|         3|Female| 20|      16000.0|                     6|          2666.6666666666665|            1|          36|\n",
      "|         4|Female| 23|      16000.0|                    77|           207.7922077922078|            1|        5929|\n",
      "|         5|Female| 31|      17000.0|                    40|                       425.0|            1|        1600|\n",
      "|         6|Female| 22|      17000.0|                    76|          223.68421052631578|            1|        5776|\n",
      "|         7|Female| 35|      18000.0|                     6|                      3000.0|            1|          36|\n",
      "|         8|Female| 23|      18000.0|                    94|          191.48936170212767|            1|        8836|\n",
      "|         9|  Male| 64|      19000.0|                     3|           6333.333333333333|            0|           9|\n",
      "|        10|Female| 30|      19000.0|                    72|           263.8888888888889|            1|        5184|\n",
      "|        11|  Male| 67|      19000.0|                    14|           1357.142857142857|            0|         196|\n",
      "|        12|Female| 35|      19000.0|                    99|          191.91919191919192|            1|        9801|\n",
      "|        13|Female| 58|      20000.0|                    15|          1333.3333333333333|            1|         225|\n",
      "|        14|Female| 24|      20000.0|                    77|           259.7402597402597|            1|        5929|\n",
      "|        15|  Male| 37|      20000.0|                    13|          1538.4615384615386|            0|         169|\n",
      "|        16|  Male| 22|      20000.0|                    79|          253.16455696202533|            0|        6241|\n",
      "|        17|Female| 35|      21000.0|                    35|                       600.0|            1|        1225|\n",
      "|        18|  Male| 20|      21000.0|                    66|           318.1818181818182|            0|        4356|\n",
      "|        19|  Male| 52|      23000.0|                    29|           793.1034482758621|            0|         841|\n",
      "|        20|Female| 35|      23000.0|                    98|          234.69387755102042|            1|        9604|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('Annual Income', 'Annual_Income')\n",
    "df = df.withColumnRenamed('udf_test', 'spend_square')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "Spark SQL is a new module in Spark which integrates relational processing with Spark’s functional programming API. It supports querying data either via SQL or via the Hive Query Language.\n",
    "\n",
    "For those of you familiar with RDBMS, Spark SQL will be an easy transition from your earlier tools where you can extend the boundaries of traditional relational data processing.\n",
    "\n",
    "Spark SQL integrates relational processing with Spark’s functional programming. It provides support for various data sources and makes it possible to weave SQL queries with code transformations thus resulting in a very powerful tool.\n",
    "\n",
    "![title](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2016/12/Spark-SQL-Architecture-Spark-SQL-Edureka-1.png)\n",
    "\n",
    "https://www.edureka.co/blog/spark-sql-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can query the data through sql by the .sql function. Remember, if you are trying to call queries locally and not through a database, then you have to create a temp view (createOrReplaceTempView). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "| 19|\n",
      "| 18|\n",
      "| 19|\n",
      "| 18|\n",
      "| 19|\n",
      "| 18|\n",
      "| 19|\n",
      "| 19|\n",
      "| 18|\n",
      "| 19|\n",
      "| 19|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"table1\")\n",
    "sql_df = sqlContext.sql(\"SELECT Age FROM table1 WHERE Age < 20\")\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "|CustomerID|Gender|Age|Annual_Income|Spending Score (1-100)|Annual_Income/Spending_Score|Gender_Binary|spend_square|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "|         1|  Male| 19|      15000.0|                    39|          384.61538461538464|            0|        1521|\n",
      "|         2|  Male| 21|      15000.0|                    81|           185.1851851851852|            0|        6561|\n",
      "|         3|Female| 20|      16000.0|                     6|          2666.6666666666665|            1|          36|\n",
      "|         4|Female| 23|      16000.0|                    77|           207.7922077922078|            1|        5929|\n",
      "|         5|Female| 31|      17000.0|                    40|                       425.0|            1|        1600|\n",
      "|         6|Female| 22|      17000.0|                    76|          223.68421052631578|            1|        5776|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM table1 WHERE Annual_Income < 18000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataframe to a bunch of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(CustomerID='1', Gender='Male', Age='19', Annual_Income=15000.0, Spending Score (1-100)='39', Annual_Income/Spending_Score=384.61538461538464, Gender_Binary=0, spend_square='1521')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = df.collect()\n",
    "lists[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby spending score and sum all the columns\n",
    "\n",
    "Let's learn how to use GroupBy and Aggregate methods on a DataFrame. GroupBy allows you to group rows together based off some column value, for example, you could group together sales data by the day the sale occured, or group repeast customer data based off the name of the customer. Once you've performed the GroupBy operation you can use an aggregate function off that data. An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+---------------------------------+------------------+\n",
      "|Spending Score (1-100)|sum(Annual_Income)|sum(Annual_Income/Spending_Score)|sum(Gender_Binary)|\n",
      "+----------------------+------------------+---------------------------------+------------------+\n",
      "|                    51|          148000.0|               2901.9607843137255|                 1|\n",
      "|                     7|           73000.0|                10428.57142857143|                 1|\n",
      "|                    15|          206000.0|               13733.333333333332|                 1|\n",
      "|                    54|          160000.0|                2962.962962962963|                 2|\n",
      "|                    11|           71000.0|                6454.545454545455|                 0|\n",
      "|                    29|           93000.0|               3206.8965517241377|                 1|\n",
      "|                    69|          191000.0|               2768.1159420289855|                 1|\n",
      "|                    42|          430000.0|               10238.095238095237|                 7|\n",
      "|                    73|          264000.0|               3616.4383561643835|                 3|\n",
      "|                    87|          105000.0|                1206.896551724138|                 2|\n",
      "|                     3|           19000.0|                6333.333333333333|                 0|\n",
      "|                    34|           72000.0|               2117.6470588235293|                 1|\n",
      "|                    59|          270000.0|                4576.271186440678|                 2|\n",
      "|                     8|          113000.0|                          14125.0|                 0|\n",
      "|                    28|          165000.0|                5892.857142857143|                 2|\n",
      "|                    22|           78000.0|               3545.4545454545455|                 1|\n",
      "|                    85|          103000.0|                1211.764705882353|                 1|\n",
      "|                    35|          233000.0|                6657.142857142857|                 3|\n",
      "|                    52|          266000.0|                5115.384615384615|                 3|\n",
      "|                    16|          198000.0|                          12375.0|                 2|\n",
      "+----------------------+------------------+---------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(F.col('Spending Score (1-100)')).sum().show() ## ignores columns that are type string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should groupby Spending Score and aggergate Age by the count as well as averaging the Annual Income. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+----------+\n",
      "|Spending Score (1-100)|avg(Annual_Income)|count(Age)|\n",
      "+----------------------+------------------+----------+\n",
      "|                    51|49333.333333333336|         3|\n",
      "|                     7|           73000.0|         1|\n",
      "|                    15| 68666.66666666667|         3|\n",
      "|                    54|53333.333333333336|         3|\n",
      "|                    11|           71000.0|         1|\n",
      "|                    29|           46500.0|         2|\n",
      "|                    69|           95500.0|         2|\n",
      "|                    42|           53750.0|         8|\n",
      "|                    73|           44000.0|         6|\n",
      "|                    87|           52500.0|         2|\n",
      "|                     3|           19000.0|         1|\n",
      "|                    34|           72000.0|         1|\n",
      "|                    59|           54000.0|         5|\n",
      "|                     8|          113000.0|         1|\n",
      "|                    28|           82500.0|         2|\n",
      "|                    22|           78000.0|         1|\n",
      "|                    85|          103000.0|         1|\n",
      "|                    35|           46600.0|         5|\n",
      "|                    52|           53200.0|         5|\n",
      "|                    16|           99000.0|         2|\n",
      "+----------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(F.col('Spending Score (1-100)')).agg({\"Age\":\"count\", \"Annual_Income\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts, means, max, Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------------------------------+------------------+\n",
      "|Gender|avg(Annual_Income)|avg(Annual_Income/Spending_Score)|avg(Gender_Binary)|\n",
      "+------+------------------+---------------------------------+------------------+\n",
      "|Female|           59250.0|               1713.7840423847515|               1.0|\n",
      "|  Male| 62227.27272727273|                4237.268560481555|               0.0|\n",
      "+------+------------------+---------------------------------+------------------+\n",
      "\n",
      "+------+-----+\n",
      "|Gender|count|\n",
      "+------+-----+\n",
      "|Female|  112|\n",
      "|  Male|   88|\n",
      "+------+-----+\n",
      "\n",
      "+------+------------------+---------------------------------+------------------+\n",
      "|Gender|max(Annual_Income)|max(Annual_Income/Spending_Score)|max(Gender_Binary)|\n",
      "+------+------------------+---------------------------------+------------------+\n",
      "|Female|          126000.0|                          15000.0|                 1|\n",
      "|  Male|          137000.0|                          78000.0|                 0|\n",
      "+------+------------------+---------------------------------+------------------+\n",
      "\n",
      "+------+------------------+---------------------------------+------------------+\n",
      "|Gender|min(Annual_Income)|min(Annual_Income/Spending_Score)|min(Gender_Binary)|\n",
      "+------+------------------+---------------------------------+------------------+\n",
      "|Female|           16000.0|               191.48936170212767|                 1|\n",
      "|  Male|           15000.0|                185.1851851851852|                 0|\n",
      "+------+------------------+---------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Gender\").mean().show()\n",
    "df.groupBy(\"Gender\").count().show()\n",
    "df.groupBy(\"Gender\").max().show()\n",
    "df.groupBy(\"Gender\").min().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OrderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "|CustomerID|Gender|Age|Annual_Income|Spending Score (1-100)|Annual_Income/Spending_Score|Gender_Binary|spend_square|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "|         1|  Male| 19|      15000.0|                    39|          384.61538461538464|            0|        1521|\n",
      "|         2|  Male| 21|      15000.0|                    81|           185.1851851851852|            0|        6561|\n",
      "|         3|Female| 20|      16000.0|                     6|          2666.6666666666665|            1|          36|\n",
      "|         4|Female| 23|      16000.0|                    77|           207.7922077922078|            1|        5929|\n",
      "|         5|Female| 31|      17000.0|                    40|                       425.0|            1|        1600|\n",
      "|         6|Female| 22|      17000.0|                    76|          223.68421052631578|            1|        5776|\n",
      "|         7|Female| 35|      18000.0|                     6|                      3000.0|            1|          36|\n",
      "|         8|Female| 23|      18000.0|                    94|          191.48936170212767|            1|        8836|\n",
      "|        12|Female| 35|      19000.0|                    99|          191.91919191919192|            1|        9801|\n",
      "|        10|Female| 30|      19000.0|                    72|           263.8888888888889|            1|        5184|\n",
      "|         9|  Male| 64|      19000.0|                     3|           6333.333333333333|            0|           9|\n",
      "|        11|  Male| 67|      19000.0|                    14|           1357.142857142857|            0|         196|\n",
      "|        14|Female| 24|      20000.0|                    77|           259.7402597402597|            1|        5929|\n",
      "|        15|  Male| 37|      20000.0|                    13|          1538.4615384615386|            0|         169|\n",
      "|        13|Female| 58|      20000.0|                    15|          1333.3333333333333|            1|         225|\n",
      "|        16|  Male| 22|      20000.0|                    79|          253.16455696202533|            0|        6241|\n",
      "|        18|  Male| 20|      21000.0|                    66|           318.1818181818182|            0|        4356|\n",
      "|        17|Female| 35|      21000.0|                    35|                       600.0|            1|        1225|\n",
      "|        19|  Male| 52|      23000.0|                    29|           793.1034482758621|            0|         841|\n",
      "|        20|Female| 35|      23000.0|                    98|          234.69387755102042|            1|        9604|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "|CustomerID|Gender|Age|Annual_Income|Spending Score (1-100)|Annual_Income/Spending_Score|Gender_Binary|spend_square|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "|       199|  Male| 32|     137000.0|                    18|           7611.111111111111|            0|         324|\n",
      "|       200|  Male| 30|     137000.0|                    83|          1650.6024096385543|            0|        6889|\n",
      "|       197|Female| 45|     126000.0|                    28|                      4500.0|            1|         784|\n",
      "|       198|  Male| 32|     126000.0|                    74|          1702.7027027027027|            0|        5476|\n",
      "|       195|Female| 47|     120000.0|                    16|                      7500.0|            1|         256|\n",
      "|       196|Female| 35|     120000.0|                    79|           1518.987341772152|            1|        6241|\n",
      "|       193|  Male| 33|     113000.0|                     8|                     14125.0|            0|          64|\n",
      "|       194|Female| 38|     113000.0|                    91|          1241.7582417582419|            1|        8281|\n",
      "|       192|Female| 32|     103000.0|                    69|          1492.7536231884058|            1|        4761|\n",
      "|       190|Female| 36|     103000.0|                    85|           1211.764705882353|            1|        7225|\n",
      "|       189|Female| 41|     103000.0|                    17|           6058.823529411765|            1|         289|\n",
      "|       191|Female| 34|     103000.0|                    23|           4478.260869565217|            1|         529|\n",
      "|       187|Female| 54|     101000.0|                    24|           4208.333333333333|            1|         576|\n",
      "|       188|  Male| 28|     101000.0|                    68|          1485.2941176470588|            0|        4624|\n",
      "|       185|Female| 41|      99000.0|                    39|          2538.4615384615386|            1|        1521|\n",
      "|       186|  Male| 30|      99000.0|                    97|          1020.6185567010309|            0|        9409|\n",
      "|       184|Female| 29|      98000.0|                    88|          1113.6363636363637|            1|        7744|\n",
      "|       183|  Male| 46|      98000.0|                    15|           6533.333333333333|            0|         225|\n",
      "|       181|Female| 37|      97000.0|                    32|                     3031.25|            1|        1024|\n",
      "|       182|Female| 32|      97000.0|                    86|           1127.906976744186|            1|        7396|\n",
      "+----------+------+---+-------------+----------------------+----------------------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"Annual_Income\").show()\n",
    "df.orderBy(df[\"Annual_Income\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this link for more info on other methods: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module\n",
    "\n",
    "Not all methods need a groupby call, instead you can just call the generalized .agg() method, that will call the aggregate across all rows in the dataframe column specified. It can take in arguments as a single column, or create multiple aggregate calls all at once using dictionary notation.\n",
    "\n",
    "There are a variety of functions you can import from pyspark.sql.functions. Check out the documentation for the full list available: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number,dayofmonth,hour,dayofyear,month,year,weekofyear,date_format\n",
    "date_df = spark.read.csv(\"maprfs:///datalake/optum/optuminsight/sandbox3/dsu/nilay_bhatt/appl_stock.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "|               6|\n",
      "|               7|\n",
      "|               8|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.select(dayofmonth(date_df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dayofyear(Date)|\n",
      "+---------------+\n",
      "|              4|\n",
      "|              5|\n",
      "|              6|\n",
      "|              7|\n",
      "|              8|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.select(dayofyear(date_df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|Year|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|2010|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|2010|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.withColumn(\"Year\",year(date_df['Date'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark3 - SB3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
